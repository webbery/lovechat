{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba as jb\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_corpus = pd.read_csv('./qa_corpus.csv')\n",
    "qa_corpus = pd.read_csv('./raw_chat_corpus/qingyun-11w/12万对话语料青云库.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>南京在哪里</td>\n",
       "      <td>在这里了</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>咋死???红烧还是爆炒</td>\n",
       "      <td>哦了哦了哦了,咱聊点别的吧</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>你个小骚货，哥哥的巴操你爽不爽？</td>\n",
       "      <td>不要这样说嘛！很不文明哦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>额麻麻怎么会有那玩意儿</td>\n",
       "      <td>无法理解您的话，获取帮助请发送 help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>孩纸,新年快乐</td>\n",
       "      <td>{r+}同乐同乐，大家一起乐~</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           question                answer\n",
       "0             南京在哪里                  在这里了\n",
       "1       咋死???红烧还是爆炒         哦了哦了哦了,咱聊点别的吧\n",
       "2  你个小骚货，哥哥的巴操你爽不爽？          不要这样说嘛！很不文明哦\n",
       "3       额麻麻怎么会有那玩意儿  无法理解您的话，获取帮助请发送 help\n",
       "4           孩纸,新年快乐       {r+}同乐同乐，大家一起乐~"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117528"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_corpus['question']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 对quest和answer做预处理，求词向量和句向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op Range in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Cast in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op HashTableV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op LookupTableImportV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op HashTableV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op TruncatedNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Add in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op MutexV2 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomUniform in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Sub in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Add in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "import cut_sentence as cs\n",
    "import hanlp\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = hanlp.load('CTB6_CONVSEG')\n",
    "\n",
    "def cut_sentence(sentence):\n",
    "    cleaned_data = ''.join(re.findall(r'[\\u4e00-\\u9fa5]', sentence))\n",
    "    return tokenizer(cleaned_data)\n",
    "\n",
    "# cut_sentence('你丫再滚个我看看')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/117528 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\yuanwb\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 2.588 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|████████████████████████████████████████████████████████████████████████| 117528/117528 [00:48<00:00, 2443.02it/s]\n"
     ]
    }
   ],
   "source": [
    "all_sentence = []\n",
    "all_questions = []\n",
    "for idx in tqdm(range(len(qa_corpus))):\n",
    "    q = qa_corpus['question'][idx]\n",
    "    if isinstance(q,str):\n",
    "        question = cs.segment(q,'arr')\n",
    "        all_sentence.append(question)\n",
    "        all_questions.append(question)\n",
    "    a = qa_corpus['answer'][idx]\n",
    "    if isinstance(a,str):\n",
    "        answer = cs.segment(a,'arr')\n",
    "        all_sentence.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用fastText求所有的词向量\n",
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(all_sentence,size=4, window=3, min_count=1, iter=10,min_n = 3 , max_n = 6,word_ngrams = 0)\n",
    "# model.save(\"fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 做句向量\n",
    "X_sentences= []\n",
    "new_sentences = []\n",
    "for idx in range(len(all_questions)):\n",
    "    sentence = all_questions[idx]\n",
    "    if len(sentence)>0:\n",
    "        vec = sum([model.wv[word] for word in sentence])/len(sentence)\n",
    "        X_sentences.append(vec)\n",
    "        new_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 意图识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 意图识别的目标是将问题分类\n",
    "+ 先对语料库中的问题进行聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means的K能否用随机行走的方式来得到？\n",
    "# 随机从一个点出发，寻找离这个点最近距离的点，然后移动到最近点，接着从这个点继续移动到下一个最近点，这条路径记为L；\n",
    "# 寻找一个合适的K，使得K个团的L总和最小，并且每个K团之间的距离最大（尽可能分开）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用K-mean聚类\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 5\n",
    "kmean = KMeans(n_clusters=n_clusters)\n",
    "kmean.fit(X_sentences)\n",
    "print(\"kmean: k={}, cost={}\".format(n_clusters, int(kmean.score(X_sentences))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "labels = kmean.labels_\n",
    "centers = kmean.cluster_centers_\n",
    "X_plot = np.array(X_sentences)\n",
    "pca = PCA(n_components=2)   #降到2维\n",
    "pca.fit(X_plot)                  #训练\n",
    "newX=pca.fit_transform(X_plot)   #降维后的数据\n",
    "\n",
    "fig = plt.figure()\n",
    "# ax = fig.gca(projection='3d')\n",
    "ax = fig.gca()\n",
    "ax.scatter(newX[:, 0], newX[:, 1], c=labels, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试一下，输入句子属于哪一类\n",
    "input = '你好'\n",
    "x_quest = cs.segment(input,'arr')\n",
    "x_vec = sum([model.wv[word] for word in x_quest])/len(x_quest)\n",
    "\n",
    "print('class: ',kmean.predict([x_vec]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(kmean , 'km.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语义相似度判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = joblib.load('km.pkl')\n",
    "print(kmean.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(model,sentence):\n",
    "    return model.predict([vectorize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "class DataSpace():\n",
    "    def __init__(self,classes_data):\n",
    "        '''\n",
    "        构建KD树\n",
    "        '''\n",
    "        points = len(classes_data)\n",
    "        leaf = points/2-1\n",
    "        self.tree = KDTree(classes_data,leaf)\n",
    "        \n",
    "    def __vectorize__(self,sentence):\n",
    "        x_quest = cs.segment(sentence,'arr')\n",
    "        return [sum([model.wv[word] for word in x_quest])/len(x_quest)]\n",
    "    \n",
    "    def similarity(self,sentence,topk=10):\n",
    "        '''\n",
    "        从KD树中找到最相似的k个数据\n",
    "        '''\n",
    "        v = self.__vectorize__(sentence)\n",
    "        dist, ind = self.tree.query(v,k=topk)\n",
    "        return (dist,ind)\n",
    "    \n",
    "    def save(self,filename):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = DataSpace(np.array(X_sentences)[kmean.labels_==0])\n",
    "sentence1 = np.array(new_sentences)[kmean.labels_==0]\n",
    "data1 = DataSpace(np.array(X_sentences)[kmean.labels_==1])\n",
    "sentence2 = np.array(new_sentences)[kmean.labels_==1]\n",
    "data2 = DataSpace(np.array(X_sentences)[kmean.labels_==2])\n",
    "sentence3 = np.array(new_sentences)[kmean.labels_==2]\n",
    "data3 = DataSpace(np.array(X_sentences)[kmean.labels_==3])\n",
    "sentence4 = np.array(new_sentences)[kmean.labels_==3]\n",
    "data4 = DataSpace(np.array(X_sentences)[kmean.labels_==4])\n",
    "sentence5 = np.array(new_sentences)[kmean.labels_==4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ind = data2.similarity('你好')\n",
    "sentence3[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 布尔搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#建立词汇-文档表\n",
    "class BoolSearch():\n",
    "    def __init__(self,docs=None):\n",
    "        if docs==None: return\n",
    "        self.words={}\n",
    "        #二进制位数\n",
    "        self.bits = len(docs)\n",
    "        self.keys = set()\n",
    "        for indx in tqdm(range(self.bits)):\n",
    "            for word in docs[indx]: self.keys.add(word)\n",
    "                    \n",
    "        for word in tqdm(self.keys):\n",
    "            for indx in range(self.bits):\n",
    "                if word not in self.words: self.words[word]=0\n",
    "                self.words[word] <<= 1\n",
    "                if word in docs[indx]:\n",
    "                    self.words[word] |= 1\n",
    "#         print(self.words)\n",
    "\n",
    "    def __itr_dict__(self,doc):\n",
    "        # 1.先遍历词典,判断词典中的词是否在新文档中\n",
    "        for word in self.words.keys():\n",
    "        # 2.如果在文档中\n",
    "            self.words[word] <<= 1\n",
    "            if word in doc:\n",
    "        # 3.就向左移位并加一\n",
    "                self.words[word] |= 1\n",
    "        # 4.否则只向左移位\n",
    "        # 5.然后从文档中移除这个词\n",
    "                while word in doc: doc.remove(word)\n",
    "                \n",
    "    def __itr_doc__(self,doc):\n",
    "        # 6.接着遍历文档中剩下的词,重复步骤3\n",
    "        for word in doc:\n",
    "            self.words[word] = 0\n",
    "            self.words[word] <<= 1\n",
    "            self.words[word] |= 1\n",
    "                    \n",
    "    def __all__(self,n):\n",
    "        num = 1\n",
    "        for i in range(n-1):\n",
    "            num <<=1\n",
    "            num |= 1\n",
    "        return num\n",
    "    \n",
    "    def search(self,input):\n",
    "        result = self.__all__(self.bits)\n",
    "#         print(self.bits,result)\n",
    "        for word in input:\n",
    "            if not self.words.__contains__(word): continue\n",
    "            result &= self.words[word]\n",
    "        return result\n",
    "    \n",
    "    def show(self):\n",
    "        print(self.words)\n",
    "        \n",
    "    def get_indexes(self,input):\n",
    "        result = self.search(input)\n",
    "        indexes = []\n",
    "        for indx in range(self.bits):\n",
    "            if result & 1: indexes.append(self.bits-indx -1)\n",
    "            result >>=1\n",
    "        return indexes\n",
    "    \n",
    "    def save(self,filepath):\n",
    "        with open(filepath, 'w') as fw:\n",
    "            json.dump({'bits':self.bits,'words':self.words},fw)\n",
    "        \n",
    "    def load(self,filepath):\n",
    "        with open(filepath,'r') as f:\n",
    "            s = json.load(f)\n",
    "            self.bits = s['bits']\n",
    "            self.words = s['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 117527/117527 [00:00<00:00, 421696.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 39369/39369 [3:46:05<00:00,  2.90it/s]\n"
     ]
    }
   ],
   "source": [
    "bool_search = BoolSearch(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_search.save('search_answer.jsn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
